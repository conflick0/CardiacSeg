{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 682,
     "status": "ok",
     "timestamp": 1675665139001,
     "user": {
      "displayName": "28",
      "userId": "04211398539985590831"
     },
     "user_tz": -480
    },
    "id": "9pI2mwskp9f7",
    "outputId": "6a438c4d-1e63-4b82-d042-5241dbba2f71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Apr 12 11:00:04 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.48.07    Driver Version: 515.48.07    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:0D:00.0 Off |                  N/A |\n",
      "| 23%   25C    P8     8W / 250W |   2625MiB / 11264MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 64087,
     "status": "ok",
     "timestamp": 1675666149682,
     "user": {
      "displayName": "28",
      "userId": "04211398539985590831"
     },
     "user_tz": -480
    },
    "id": "OFPs3KB4p4RL",
    "outputId": "5bd274b7-748f-46e8-ee0e-46317e8f4a9b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "workspace_dir = '/nfs/Workspace/CardiacSeg'\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import PurePath\n",
    "from functools import partial\n",
    "\n",
    "sys.path.append(workspace_dir)\n",
    "\n",
    "from torchsummaryX import summary\n",
    "\n",
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from monai.transforms import (\n",
    "    Compose,\n",
    "    AddChanneld,\n",
    "    EnsureChannelFirstd,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    SqueezeDimd,\n",
    "    LoadImage,\n",
    "    Spacingd,\n",
    "    ScaleIntensityRanged,\n",
    "    RandCropByPosNegLabeld,\n",
    "    ToNumpyd,\n",
    ")\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.visualize import GradCAM\n",
    "from monailabel.transform.post import Restored\n",
    "\n",
    "\n",
    "from datasets.chgh_dataset import get_data_dicts\n",
    "\n",
    "from data_utils.data_loader_utils import load_data_dict_json\n",
    "from data_utils.dataset import get_infer_data\n",
    "from data_utils.utils import get_pid_by_file\n",
    "from data_utils.io import load_json\n",
    "from data_utils.visualization import show_img_lbl, show_img_lbl_pred, show_img_lbl_preds, show_img_lbl_preds_overlap\n",
    "\n",
    "from runners.inferer import run_infering\n",
    "from expers.infer_utils import get_tune_model_dir, get_data_path, get_pred_path\n",
    "from expers.args import get_parser\n",
    "\n",
    "from networks.network import network\n",
    "from networks.networkx.blocks.cbam import CBAM\n",
    "from networks.networkx.blocks.ham import HAM\n",
    "from networks.ssl_head import SSLHead\n",
    "\n",
    "# sync python module\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading results from /nfs/Workspace/CardiacSeg/exps/exps/unetcnx_x3_2_2_a5/chgh/tune_results/t_4...\n",
      "2023-04-12 11:00:11,277 - No `self.trials`. Drawing logdirs from checkpoint file. This may result in some information that is out of sync, as checkpointing is periodic.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing in the experiment's `trainable` will be a required argument to `Tuner.restore` starting from version 2.5. Please specify the trainable to avoid this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best trial 6d320_00000: \n",
      "config: {'exp': {'exp': 'exp_b7_9_x3_2_2_a5'}}\n",
      "tt_dice: 0.8903141\n",
      "tt_hd95: 4.954921004130534\n",
      "best log dir: /nfs/Workspace/CardiacSeg/exps/exps/unetcnx_x3_2_2_a5/chgh/tune_results/t_4/main_6d320_00000_0_exp=exp_exp_b7_9_x3_2_2_a5_2023-03-29_03-54-38\n",
      "\n",
      "best model: /nfs/Workspace/CardiacSeg/exps/exps/unetcnx_x3_2_2_a5/chgh/tune_results/t_4/main_6d320_00000_0_exp=exp_exp_b7_9_x3_2_2_a5_2023-03-29_03-54-38/models/best_model.pth\n",
      "infer dir: /nfs/Workspace/CardiacSeg/exps/exps/unetcnx_x3_2_2_a5/chgh/infers/t_4\n",
      "/nfs/Workspace/CardiacSeg/dataset/chgh/dataset_2/pid_1000/pid_1000.nii.gz\n",
      "/nfs/Workspace/CardiacSeg/dataset/chgh/dataset_2/pid_1000/pid_1000_gt.nii.gz\n",
      "/nfs/Workspace/CardiacSeg/exps/exps/unetcnx_x3_2_2_a5/chgh/tune_results/infers/t_4/pid_1000.nii.gz\n"
     ]
    }
   ],
   "source": [
    "model_name = 'unetcnx_x3_2_2_a5' #'unetcnx_x3_2_2'\n",
    "data_name = 'chgh'\n",
    "sub_data_dir_name = 'dataset_2'\n",
    "exp_name = 't_4'\n",
    "data_dict_file_name = 'exp_2_2.json' #'exp_2_2.json' exp_b7_9\n",
    "\n",
    "root_exp_dir = os.path.join(\n",
    "    workspace_dir, \n",
    "    'exps',\n",
    "    'exps',\n",
    "    model_name,\n",
    "    data_name,\n",
    "    'tune_results'\n",
    ")\n",
    "\n",
    "root_data_dir = os.path.join(\n",
    "    workspace_dir, \n",
    "    'dataset',\n",
    "    data_name\n",
    ")\n",
    "\n",
    "data_dir = os.path.join(root_data_dir, sub_data_dir_name)\n",
    "\n",
    "model_dir = get_tune_model_dir(root_exp_dir, exp_name)\n",
    "\n",
    "best_checkpoint = os.path.join(model_dir, 'best_model.pth')\n",
    "final_checkpoint = os.path.join(model_dir, 'final_model.pth')\n",
    "\n",
    "infer_dir = os.path.join(\n",
    "    workspace_dir, \n",
    "    'exps',\n",
    "    'exps',\n",
    "    model_name,\n",
    "    data_name,\n",
    "    'infers',\n",
    "    exp_name,\n",
    ")\n",
    "\n",
    "print('\\nbest model:',best_checkpoint)\n",
    "print('infer dir:',infer_dir)\n",
    "\n",
    "pid = 'pid_1000'\n",
    "data_dict = get_data_path(data_dir, pid)\n",
    "data_dict['pred'] = get_pred_path(root_exp_dir, exp_name, data_dict['image'])\n",
    "\n",
    "img_pth = data_dict['image']\n",
    "lbl_pth = data_dict['label'] \n",
    "print(img_pth)\n",
    "print(lbl_pth)\n",
    "print(data_dict['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1409,
     "status": "ok",
     "timestamp": 1675653198391,
     "user": {
      "displayName": "29",
      "userId": "05747767364965554341"
     },
     "user_tz": -480
    },
    "id": "PA5zeoZCqqSS",
    "outputId": "0523d60e-603c-4eb5-d433-ced1f2468477"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is available\n",
      "model: unetcnx_x3_2_2_a5\n",
      "[4, 4, 8, 4]\n",
      "=> loaded checkpoint '/nfs/Workspace/CardiacSeg/exps/exps/unetcnx_x3_2_2_a5/chgh/tune_results/t_4/main_6d320_00000_0_exp=exp_exp_b7_9_x3_2_2_a5_2023-03-29_03-54-38/models/best_model.pth')\n"
     ]
    }
   ],
   "source": [
    "args = get_parser([])\n",
    "args.model_name=model_name\n",
    "args.data_name=data_name\n",
    "args.data_dir=data_dir\n",
    "args.model_dir=model_dir\n",
    "args.infer_dir=infer_dir\n",
    "args.checkpoint=best_checkpoint\n",
    "\n",
    "args.out_channels=2 \n",
    "args.patch_size=4 \n",
    "args.drop_rate=0.4 \n",
    "args.depths=[4, 4, 8, 4] \n",
    "args.a_min = -42\n",
    "args.a_max = 423\n",
    "args.space_x = 0.7\n",
    "args.space_y = 0.7\n",
    "args.space_z = 1.0\n",
    "args.roi_x = 128\n",
    "args.roi_y = 128\n",
    "args.roi_z = 128\n",
    "args.infer_overlap = 0.25\n",
    "args.sw_batch_size = 2\n",
    "data_dicts = [{\n",
    "    'image': img_pth,\n",
    "    'label': lbl_pth\n",
    "}]\n",
    "data_dicts\n",
    "\n",
    "# device\n",
    "if torch.cuda.is_available():\n",
    "    print(\"cuda is available\")\n",
    "    args.device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"cuda is not available\")\n",
    "    args.device = torch.device(\"cpu\")\n",
    "\n",
    "# model\n",
    "model = network(args.model_name, args)\n",
    "\n",
    "# check point\n",
    "if args.checkpoint is not None:\n",
    "    checkpoint = torch.load(args.checkpoint, map_location=\"cpu\")\n",
    "    # load model\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    # load check point epoch and best acc\n",
    "\n",
    "    print(\n",
    "      \"=> loaded checkpoint '{}')\"\\\n",
    "      .format(args.checkpoint)\n",
    "    )\n",
    "\n",
    "# post transforom\n",
    "post_transform = Compose([\n",
    "    Orientationd(keys=['pred'], axcodes=\"LPS\"),\n",
    "    ToNumpyd(keys=['pred']),\n",
    "    Restored(keys=['pred'], ref_image=\"image\")\n",
    "])\n",
    "\n",
    "# inferer\n",
    "model_inferer = partial(\n",
    "    sliding_window_inference,\n",
    "    roi_size=[args.roi_x, args.roi_y, args.roi_z],\n",
    "    sw_batch_size=args.sw_batch_size,\n",
    "    predictor=model,\n",
    "    overlap=args.infer_overlap,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<class 'monai.transforms.utility.array.AddChannel'>: Class `AddChannel` has been deprecated since version 0.8. please use MetaTensor data type and monai.transforms.EnsureChannelFirst instead.\n"
     ]
    }
   ],
   "source": [
    "# transforms\n",
    "transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        AddChanneld(keys=[\"image\", \"label\"]),\n",
    "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "        Spacingd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            pixdim=(args.space_x, args.space_y, args.space_z),\n",
    "            mode=(\"bilinear\", \"nearest\"),\n",
    "        ),\n",
    "        ScaleIntensityRanged(\n",
    "            keys=[\"image\"], a_min=args.a_min, a_max=args.a_max, b_min=args.b_min, b_max=args.b_max, clip=True\n",
    "        ),\n",
    "        RandCropByPosNegLabeld(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            label_key=\"label\",\n",
    "            spatial_size=(args.roi_x, args.roi_y, args.roi_z),\n",
    "            pos=1,\n",
    "            neg=1,\n",
    "            num_samples=1,\n",
    "            image_key=\"image\",\n",
    "            image_threshold=0,\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_ds = transforms(data_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 2 but got size 3 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m cam \u001b[38;5;241m=\u001b[39m GradCAM(nn_module\u001b[38;5;241m=\u001b[39mmodel, target_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mcam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrand\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m96\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m96\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m96\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/monai/visualize/class_activation_maps.py:381\u001b[0m, in \u001b[0;36mGradCAM.__call__\u001b[0;34m(self, x, class_idx, layer_idx, retain_graph, **kwargs)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, class_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, layer_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    368\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;124;03m    Compute the activation map with upsampling and postprocessing.\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;124;03m        activation maps\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 381\u001b[0m     acti_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_upsample_and_post_process(acti_map, x)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/monai/visualize/class_activation_maps.py:360\u001b[0m, in \u001b[0;36mGradCAM.compute_map\u001b[0;34m(self, x, class_idx, retain_graph, layer_idx, **kwargs)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_map\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, class_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, layer_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 360\u001b[0m     _, acti, grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    361\u001b[0m     acti, grad \u001b[38;5;241m=\u001b[39m acti[layer_idx], grad[layer_idx]\n\u001b[1;32m    362\u001b[0m     b, c, \u001b[38;5;241m*\u001b[39mspatial \u001b[38;5;241m=\u001b[39m grad\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/monai/visualize/class_activation_maps.py:131\u001b[0m, in \u001b[0;36mModelWithHooks.__call__\u001b[0;34m(self, x, class_idx, retain_graph, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtraining\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m--> 131\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_idx \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m class_idx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m class_idx\n\u001b[1;32m    133\u001b[0m acti, grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/nfs/Workspace/CardiacSeg/networks/networkx/unetcnx_x3_2_2_a5.py:147\u001b[0m, in \u001b[0;36mUNETCNX_X3_2_2_A5.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    144\u001b[0m skip_enc3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_encoder3(enc3)\n\u001b[1;32m    145\u001b[0m skip_enc4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_encoder4(enc4)\n\u001b[0;32m--> 147\u001b[0m dec4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder5\u001b[49m\u001b[43m(\u001b[49m\u001b[43mskip_enc4\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_enc3\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m dec3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder4(dec4, skip_enc2)\n\u001b[1;32m    150\u001b[0m dec2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder3(dec3, skip_enc1)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/monai/networks/blocks/unetr_block.py:82\u001b[0m, in \u001b[0;36mUnetrUpBlock.forward\u001b[0;34m(self, inp, skip)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inp, skip):\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;66;03m# number of channels for skip should equals to out_channels\u001b[39;00m\n\u001b[1;32m     81\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransp_conv(inp)\n\u001b[0;32m---> 82\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_block(out)\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 2 but got size 3 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "cam = GradCAM(nn_module=model, target_layers=\"encoder1\")\n",
    "result = cam(x=torch.rand((1, 1, 96, 96, 96)).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 16384.00 GiB (GPU 0; 10.92 GiB total capacity; 5.70 GiB already allocated; 1.85 GiB free; 5.76 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m cam \u001b[38;5;241m=\u001b[39m GradCAM(nn_module\u001b[38;5;241m=\u001b[39mmodel, target_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# cam = monai.visualize.GradCAMpp(nn_module=model_3d, target_layers=\"class_layers.relu\")\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moriginal feature shape\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m----> 9\u001b[0m     \u001b[43mcam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_map_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mwin_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupsampled feature shape\u001b[39m\u001b[38;5;124m\"\u001b[39m, [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(win_size))\n\u001b[1;32m     13\u001b[0m occ_sens \u001b[38;5;241m=\u001b[39m monai\u001b[38;5;241m.\u001b[39mvisualize\u001b[38;5;241m.\u001b[39mOcclusionSensitivity(nn_module\u001b[38;5;241m=\u001b[39mmodel, mask_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m96\u001b[39m, n_batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m48\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/monai/visualize/class_activation_maps.py:190\u001b[0m, in \u001b[0;36mCAMBase.feature_map_size\u001b[0;34m(self, input_size, device, layer_idx, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeature_map_size\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_size, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, layer_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    180\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;124;03m    Computes the actual feature map size given `nn_module` and the target_layer name.\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;124;03m        shape of the actual feature map.\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/monai/visualize/class_activation_maps.py:360\u001b[0m, in \u001b[0;36mGradCAM.compute_map\u001b[0;34m(self, x, class_idx, retain_graph, layer_idx, **kwargs)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_map\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, class_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, layer_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 360\u001b[0m     _, acti, grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    361\u001b[0m     acti, grad \u001b[38;5;241m=\u001b[39m acti[layer_idx], grad[layer_idx]\n\u001b[1;32m    362\u001b[0m     b, c, \u001b[38;5;241m*\u001b[39mspatial \u001b[38;5;241m=\u001b[39m grad\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/monai/visualize/class_activation_maps.py:137\u001b[0m, in \u001b[0;36mModelWithHooks.__call__\u001b[0;34m(self, x, class_idx, retain_graph, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m     acti \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivations[layer] \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_layers)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_backward:\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscore \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_idx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscore\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mbackward(retain_graph\u001b[38;5;241m=\u001b[39mretain_graph)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/monai/visualize/class_activation_maps.py:126\u001b[0m, in \u001b[0;36mModelWithHooks.class_score\u001b[0;34m(self, logits, class_idx)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclass_score\u001b[39m(\u001b[38;5;28mself\u001b[39m, logits: torch\u001b[38;5;241m.\u001b[39mTensor, class_idx: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlogits\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 16384.00 GiB (GPU 0; 10.92 GiB total capacity; 5.70 GiB already allocated; 1.85 GiB free; 5.76 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "device = args.device\n",
    "win_size = (args.roi_x, args.roi_y, args.roi_z)\n",
    "\n",
    "# cam = monai.visualize.CAM(nn_module=model_3d, target_layers=\"class_layers.relu\", fc_layers=\"class_layers.out\")\n",
    "cam = GradCAM(nn_module=model, target_layers=\"encoder1\")\n",
    "# cam = monai.visualize.GradCAMpp(nn_module=model_3d, target_layers=\"class_layers.relu\")\n",
    "print(\n",
    "    \"original feature shape\",\n",
    "    cam.feature_map_size([1, 1] + list(win_size), device),\n",
    ")\n",
    "print(\"upsampled feature shape\", [1, 1] + list(win_size))\n",
    "\n",
    "occ_sens = monai.visualize.OcclusionSensitivity(nn_module=model, mask_size=96, n_batch=1, stride=48)\n",
    "\n",
    "# For occlusion sensitivity, inference must be run many times. Hence, we can use a\n",
    "# bounding box to limit it to a 2D plane of interest (z=the_slice) where each of\n",
    "# the arguments are the min and max for each of the spatial dimensions (in this case HWD).\n",
    "\n",
    "the_slice = train_ds[0][\"image\"].shape[-1] // 2\n",
    "occ_sens_b_box = [-1, -1, -1, -1, the_slice - 1, the_slice]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms.set_random_state(42)\n",
    "n_examples = 5\n",
    "subplot_shape = [3, n_examples]\n",
    "fig, axes = plt.subplots(*subplot_shape, figsize=(25, 15), facecolor=\"white\")\n",
    "items = np.random.choice(len(train_ds), size=len(train_ds), replace=False)\n",
    "\n",
    "example = 0\n",
    "for item in items:\n",
    "    data = train_ds[item]  # this fetches training data with random augmentations\n",
    "    image, label = data[\"image\"].to(device).unsqueeze(0), data[\"label\"][1]\n",
    "    y_pred = model_3d(image)\n",
    "    pred_label = y_pred.argmax(1).item()\n",
    "    # Only display tumours images\n",
    "    if label != 1 or label != pred_label:\n",
    "        continue\n",
    "\n",
    "    img = image.detach().cpu().numpy()[..., the_slice]\n",
    "\n",
    "    name = \"actual: \"\n",
    "    name += \"lesion\" if label == 1 else \"non-lesion\"\n",
    "    name += \"\\npred: \"\n",
    "    name += \"lesion\" if pred_label == 1 else \"non-lesion\"\n",
    "    name += f\"\\nlesion: {y_pred[0,1]:.3}\"\n",
    "    name += f\"\\nnon-lesion: {y_pred[0,0]:.3}\"\n",
    "\n",
    "    # run CAM\n",
    "    cam_result = cam(x=image, class_idx=None)\n",
    "    cam_result = cam_result[..., the_slice]\n",
    "\n",
    "    # run occlusion\n",
    "    occ_result, _ = occ_sens(x=image, b_box=occ_sens_b_box)\n",
    "    occ_result = occ_result[0, pred_label][None, None, ..., -1]\n",
    "\n",
    "    for row, (im, title) in enumerate(\n",
    "        zip(\n",
    "            [img, cam_result, occ_result],\n",
    "            [name, \"CAM\", \"Occ. sens.\"],\n",
    "        )\n",
    "    ):\n",
    "        cmap = \"gray\" if row == 0 else \"jet\"\n",
    "        ax = axes[row, example]\n",
    "        if isinstance(im, torch.Tensor):\n",
    "            im = im.cpu().detach()\n",
    "        im_show = ax.imshow(im[0][0], cmap=cmap)\n",
    "\n",
    "        ax.set_title(title, fontsize=25)\n",
    "        ax.axis(\"off\")\n",
    "        fig.colorbar(im_show, ax=ax)\n",
    "\n",
    "    example += 1\n",
    "    if example == n_examples:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "fBdlQzFvr-lD"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
